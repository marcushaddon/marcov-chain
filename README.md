# marcov-chain
A class library for implementing Markov Chains
